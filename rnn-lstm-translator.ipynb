{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2f4e72",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1767199313154,
     "user": {
      "displayName": "Apb Reloaded",
      "userId": "14539421398603321875"
     },
     "user_tz": -120
    },
    "id": "ab2f4e72"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "from pickle import dump, load\n",
    "from unicodedata import normalize\n",
    "from numpy import array, argmax\n",
    "from numpy.random import rand, shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical, plot_model, pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Embedding, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dab970",
   "metadata": {
    "id": "47dab970"
   },
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bee8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11753,
     "status": "ok",
     "timestamp": 1767199324911,
     "user": {
      "displayName": "Apb Reloaded",
      "userId": "14539421398603321875"
     },
     "user_tz": -120
    },
    "id": "1f8bee8b",
    "outputId": "3621520a-914d-4411-b777-cafae424219e"
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_pairs(lines):\n",
    "\tcleaned = list()\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor pair in lines:\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\n",
    "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\t\t\tline = line.split()\n",
    "\t\t\tline = [word.lower() for word in line]\n",
    "\t\t\tline = [word.translate(table) for word in line]\n",
    "\t\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\treturn array(cleaned)\n",
    "\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "def load_clean_data(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "file1 = './content/data/fra_eng.pkl'\n",
    "file2 = './content/data/fra_eng_both.pkl'\n",
    "file3 = './content/data/fra_eng_train.pkl'\n",
    "file4 = './content/data/fra_eng_test.pkl'\n",
    "\n",
    "# Checking if all preprocessed files exist\n",
    "if os.path.exists(file1) and os.path.exists(file2) and os.path.exists(file3) and os.path.exists(file4):\n",
    "    print(\"Preprocessed files already exist. Skipping data preparation.\")\n",
    "else:\n",
    "\tfilename = './content/data/fra.txt'\n",
    "\tdoc = load_doc(filename)\n",
    "\tpairs = to_pairs(doc)\n",
    "\tcleaned_pairs = clean_pairs(pairs)\n",
    "\tsave_clean_data(cleaned_pairs, './content/data/fra_eng.pkl')\n",
    "\n",
    "\tfor i in range(100):\n",
    "\t\tprint('[%s] => [%s]' % (cleaned_pairs[i,0], cleaned_pairs[i,1]))\n",
    "\n",
    "\traw_dataset = load_clean_data('./content/data/fra_eng.pkl')\n",
    "\tn_sentences = 15000\n",
    "\tdataset = raw_dataset[:n_sentences, :]\n",
    "\tshuffle(dataset)\n",
    "\ttrain, test =  dataset[:12000], dataset[12000:]\n",
    "\n",
    "\tsave_clean_data(dataset, './content/data/fra_eng_both.pkl')\n",
    "\tsave_clean_data(train, './content/data/fra_eng_train.pkl')\n",
    "\tsave_clean_data(test, './content/data/fra_eng_test.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d11a3",
   "metadata": {
    "id": "d55d11a3"
   },
   "source": [
    "Sequence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ec5b54",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767199324923,
     "user": {
      "displayName": "Apb Reloaded",
      "userId": "14539421398603321875"
     },
     "user_tz": -120
    },
    "id": "41ec5b54"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e720ac6",
   "metadata": {
    "id": "6e720ac6"
   },
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e595e993",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1767199324933,
     "user": {
      "displayName": "Apb Reloaded",
      "userId": "14539421398603321875"
     },
     "user_tz": -120
    },
    "id": "e595e993"
   },
   "outputs": [],
   "source": [
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add((LSTM(n_units)))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add((LSTM(n_units, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7c5ed",
   "metadata": {
    "id": "73b7c5ed"
   },
   "source": [
    "English/French Tokenizers & Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc9508b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1767199325299,
     "user": {
      "displayName": "Apb Reloaded",
      "userId": "14539421398603321875"
     },
     "user_tz": -120
    },
    "id": "bc9508b5",
    "outputId": "5c3aae0a-defa-4f84-d320-033d4551866d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2888\n",
      "English Max Length: 5\n",
      "\n",
      "French Vocabulary Size: 5797\n",
      "French Max Length: 11\n"
     ]
    }
   ],
   "source": [
    "dataset = load_clean_data('./content/data/fra_eng_both.pkl')\n",
    "train = load_clean_data('./content/data/fra_eng_train.pkl')\n",
    "test = load_clean_data('./content/data/fra_eng_test.pkl')\n",
    "\n",
    "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:,0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length) + \"\\n\")\n",
    "\n",
    "fra_tokenizer = create_tokenizer(dataset[:,1])\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_length = max_length(dataset[:,1])\n",
    "print('French Vocabulary Size: %d' % fra_vocab_size)\n",
    "print('French Max Length: %d' % (fra_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9f557",
   "metadata": {
    "id": "6bd9f557"
   },
   "source": [
    "Preparing Training/Testing Data & Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "096b8482",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 2003,
     "status": "ok",
     "timestamp": 1767199327334,
     "user": {
      "displayName": "Apb Reloaded",
      "userId": "14539421398603321875"
     },
     "user_tz": -120
    },
    "id": "096b8482",
    "outputId": "fda3bb8d-9672-490e-b3d1-6fd7bb60798c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 11, 256)           1484032   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               525312    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 5, 256)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 5, 256)            525312    \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 5, 2888)          742216    \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,276,872\n",
      "Trainable params: 3,276,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainX = encode_sequences(fra_tokenizer, fra_length, train[:,1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:,0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "testX = encode_sequences(fra_tokenizer, fra_length, test[:,1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:,0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "model = define_model(fra_vocab_size, eng_vocab_size, fra_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, fra_length))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#plot_model(model, to_file='./content/model-architecture.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915971f6",
   "metadata": {
    "id": "915971f6"
   },
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb6724",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116535,
     "status": "ok",
     "timestamp": 1767199443872,
     "user": {
      "displayName": "Apb Reloaded",
      "userId": "14539421398603321875"
     },
     "user_tz": -120
    },
    "id": "5bcb6724",
    "outputId": "968cf7a5-bd54-4a9b-d125-acfdc15c132f"
   },
   "outputs": [],
   "source": [
    "filename = './content/eng-fra-model.keras'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27ad1f",
   "metadata": {
    "id": "1f27ad1f"
   },
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99e59049",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1654,
     "status": "ok",
     "timestamp": 1767199445536,
     "user": {
      "displayName": "Apb Reloaded",
      "userId": "14539421398603321875"
     },
     "user_tz": -120
    },
    "id": "99e59049",
    "outputId": "4c37550f-d57e-4087-f259-4f0de56abd6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Evaluation\n",
      "src=[jai besoin detudier], target=[i need to study], predicted=[i need to study]\n",
      "src=[tom est un heros], target=[tom is a hero], predicted=[tom is a hero]\n",
      "src=[quel gros chien], target=[what a big dog], predicted=[what a big dog]\n",
      "src=[ce sont les maths que je prefere], target=[i like math best], predicted=[i like your you]\n",
      "src=[je te crois vraiment], target=[i do believe you], predicted=[i do believe you]\n",
      "src=[je men chargerai], target=[ill handle this], predicted=[ill handle it]\n",
      "src=[je les ai soudoyees], target=[i bribed them], predicted=[i bribed them]\n",
      "src=[puisje aller au lit], target=[may i go to bed], predicted=[may i go to bed]\n",
      "src=[je ne laime pas], target=[i dont like him], predicted=[i dont like it]\n",
      "src=[jai un ranch], target=[i have a ranch], predicted=[i have a ranch]\n",
      "BLEU-1: 0.897436\n",
      "BLEU-2: 0.879575\n",
      "BLEU-3: 0.862512\n",
      "BLEU-4: 0.789582\n",
      "\n",
      "Testing Data Evaluation\n",
      "src=[jarrive a le ressentir], target=[i can feel it], predicted=[i can remember it]\n",
      "src=[jaime pecher], target=[i like fishing], predicted=[i like hiking]\n",
      "src=[battez les ufs], target=[beat the eggs], predicted=[beat some eggs]\n",
      "src=[estu prete], target=[are you ready], predicted=[are you ready]\n",
      "src=[je veux men aller], target=[i want to go], predicted=[i want to go]\n",
      "src=[je me suis amuse], target=[i did have fun], predicted=[i had some]\n",
      "src=[je craque], target=[im cracking up], predicted=[i check first]\n",
      "src=[jai echoue], target=[ive failed], predicted=[i failed]\n",
      "src=[tom begaie], target=[tom stutters], predicted=[tom is]\n",
      "src=[ils mont souhaite la bienvenue], target=[they greeted me], predicted=[they greeted me]\n",
      "BLEU-1: 0.644811\n",
      "BLEU-2: 0.529766\n",
      "BLEU-3: 0.484756\n",
      "BLEU-4: 0.305861\n"
     ]
    }
   ],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    "\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)\n",
    "\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "\tactual, predicted = list(), list()\n",
    "\tfor i, source in enumerate(sources):\n",
    "\t\t# translate encoded source text\n",
    "\t\tsource = source.reshape((1, source.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
    "\t\traw_target, raw_src = raw_dataset[i]\n",
    "\t\tif i < 10:\n",
    "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\t\tactual.append([raw_target.split()])\n",
    "\t\tpredicted.append(translation.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "model = load_model('./content/eng-fra-model.keras')\n",
    "\n",
    "print('Training Data Evaluation')\n",
    "evaluate_model(model, eng_tokenizer, trainX[:10], train[:10])\n",
    "\n",
    "print(\"\\n\" 'Testing Data Evaluation')\n",
    "evaluate_model(model, eng_tokenizer, testX[:10], test[:10])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfdml_plugin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
